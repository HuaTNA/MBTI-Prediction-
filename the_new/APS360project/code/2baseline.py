import cv2
import dlib
import numpy as np
import os
import random
from skimage.feature import hog, local_binary_pattern
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# **åˆå§‹åŒ– Dlib æ¨¡å‹**
predictor = dlib.shape_predictor("H:/CODE/APS360/Model/shape_predictor_68_face_landmarks.dat")

# **å‡å°‘è®­ç»ƒæ•°æ®**
MAX_SAMPLES_PER_CATEGORY = 200  # å…ˆç”¨ 200 å¼ /ç±» è®­ç»ƒ SVM

def extract_features(img_path):
    """ è¯»å–å›¾ç‰‡ & æå– HOG + LBP + å…³é”®ç‚¹ç‰¹å¾ """
    image = cv2.imread(img_path)
    if image is None:
        print(f"âš ï¸ æ— æ³•åŠ è½½ {img_path}ï¼Œè·³è¿‡ï¼")
        return None

    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    detector = dlib.get_frontal_face_detector()
    rects = detector(gray, 1)

    if len(rects) == 0:
        print(f"âš ï¸ æœªæ£€æµ‹åˆ°äººè„¸: {img_path}")
        return None

    shape = predictor(gray, rects[0])
    landmarks = np.array([(p.x, p.y) for p in shape.parts()]).flatten()

    # **ä¼˜åŒ– HOG ç‰¹å¾**
    hog_features = hog(gray, orientations=9, pixels_per_cell=(16, 16),
                       cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)

    # **å¢åŠ  LBP ç‰¹å¾**
    lbp_features = local_binary_pattern(gray, P=8, R=1).flatten()

    return np.hstack([hog_features, lbp_features, landmarks]) 

# **åŠ è½½æ•°æ®é›†**
data_dir = r"H:\CODE\APS360\Data\Face_emotion\Split\train"
categories = os.listdir(data_dir)

X = []
y = []
image_paths = []
labels = []

print(f"ğŸ“‚ æ‰¾åˆ° {len(categories)} ä¸ªç±»åˆ«: {categories}")
for category in tqdm(categories, desc="Processing Categories"):  
    label = categories.index(category)
    category_path = os.path.join(data_dir, category)

    img_names = os.listdir(category_path)[:MAX_SAMPLES_PER_CATEGORY]  
    for img_name in img_names:
        img_path = os.path.join(category_path, img_name)
        image_paths.append(img_path)
        labels.append(label)  

print(f"ğŸš€ ä½¿ç”¨å°æ•°æ®é›†ï¼Œæ€»å…± {len(image_paths)} å¼ å›¾ç‰‡")

# **å¤šçº¿ç¨‹å¤„ç†**
print("ğŸš€ å¼€å§‹å¹¶è¡Œæå–ç‰¹å¾...")
with ThreadPoolExecutor(max_workers=2) as executor:  
    futures = {executor.submit(extract_features, img_path): i for i, img_path in enumerate(image_paths)}

    for future in tqdm(as_completed(futures), total=len(futures), desc="Extracting Features"):
        result = future.result()
        if result is not None:
            X.append(result)
            y.append(labels[futures[future]])

print("âœ… ç‰¹å¾æå–å®Œæˆï¼")
print(f"ğŸ“Š æ ·æœ¬æ•°: {len(X)}, æ ‡ç­¾æ•°: {len(y)}")

# **æ•°æ®é›†åˆ’åˆ†**
print("ğŸ”„ å¼€å§‹åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
X = np.array(X)
y = np.array(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print("âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼")

# **è°ƒæ•´ SVM è¶…å‚æ•°**
print("ğŸš€ å¼€å§‹è®­ç»ƒ SVM...")
svm_model = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=10, gamma='scale'))

for i in tqdm(range(1), desc="Training SVM"):
    svm_model.fit(X_train, y_train)

print("âœ… SVM è®­ç»ƒå®Œæˆï¼")

# **è¯„ä¼°æ¨¡å‹**
print("ğŸ“ˆ è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡å’Œæµ‹è¯•å‡†ç¡®ç‡...")
train_acc = svm_model.score(X_train, y_train)
test_acc = svm_model.score(X_test, y_test)

print(f"ğŸ¯ Training Accuracy: {train_acc:.2%}")
print(f"ğŸ“ Testing Accuracy: {test_acc:.2%}")
